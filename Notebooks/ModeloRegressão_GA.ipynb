{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94163fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MH  NJOBS  NMACHINES  CMAXOPT OBJFUNC  CMAX  NUMGEN  TIMEEXEC\n",
      "0   GA     10          5      655      WT   837     100     10.62\n",
      "1   GA     10         10      887      WT  1504     100     13.50\n",
      "2   GA     10         10      887      WT  1653     100     13.71\n",
      "3   GA     15          5      890      WT  1020     100     27.41\n",
      "4   GA     10         10      899      WT  1501     100      9.55\n",
      "5   GA     10         10      899      WT  1560     100      9.66\n",
      "6   GA     15          5      926      WT   987     100     24.70\n",
      "7   GA     20          5     1165      WT  1624     100     69.50\n",
      "8   GA     20         10     1218      WT  1627     100     52.48\n",
      "9   GA     20          5     1222      WT  1222     100     56.67\n",
      "10  GA     20         10     1424      WT  2310     100     54.03\n",
      "11  GA     20         10     1424      WT  2421     100     58.11\n",
      "12  GA     20         15     1446      WT  2406     100     56.01\n",
      "13  GA     20         15     1446    Cmax  2482     100     54.43\n",
      "14  GA     20         15     1446      WT  2535     100     54.54\n",
      "15  GA     20         15     1446    Cmax  2540     100     56.06\n",
      "16  GA     20         10     1450    Cmax  2397     100     55.93\n",
      "17  GA     20         15     1631      WT  2677     100     54.25\n",
      "18  GA     50         10     2823      WT  2886     350    403.17\n",
      "19  GA     50         10     2823      WT  2917     350    403.91\n",
      "20  GA     50         10     2972      WT  5094     350    429.15\n",
      "21  GA     50         10     2972    Cmax  5205     350    420.70\n",
      "22  GA     50         10     2972    Cmax  5298     350    422.69\n",
      "23  GA     50         10     2972      WT  5418     350    445.13\n",
      "24  GA     50         10     2983    Cmax  5220     350    397.31\n",
      "25  GA     50         10     2983    Cmax  5291     350    408.63\n",
      "26  GA     50         10     3104      WT  5600     350    422.08\n",
      "27  GA     50         10     3104      WT  5600     350    422.08\n",
      "28  GA     50         10     3104    Cmax  5602     350    426.61\n",
      "29  GA     50         10     3104    Cmax  5617     350    409.77\n",
      "30  GA     50         10     2972      WT  5353     361    439.51\n",
      "31  GA     50         10     2983    Cmax  5272     361    428.78\n",
      "32  GA     50         10     2983      WT  5409     361    430.99\n",
      "33  GA     20          5     1207      WT  1433     106     58.52\n",
      "34  GA     20          5     1207      WT  1553     106     58.64\n",
      "35  GA     30         10     1721      WT  2016     200    156.88\n",
      "36  GA     30         10     1784      WT  2086     200    159.50\n",
      "37  GA     30         10     1784      WT  2134     192    159.48\n",
      "38  GA     30         10     1850      WT  2111     192    165.30\n",
      "39  GA     50         10     3104    Cmax  5436     349    419.61\n",
      "40  GA     20         10     1369      WT  2340     104     60.06\n"
     ]
    }
   ],
   "source": [
    "#MODELO PREDITIVO INDIVIDUAL RELATIVO AOS ALGORITMOS GENÉTICOS - MODELO DE REGRESSÃO DE AFINAÇÃO DOS PARÂMETROS --------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import xlwings as xw\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "#EXCEL - CAMINHO DO FICHEIRO\n",
    "file_path = r\"D:\\Documentos\\...\\DataSet_Regressão_GA.xlsx\"\n",
    "dados_problema= pd.read_excel(file_path)\n",
    "print (dados_problema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56292ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIVIDIR DADOS EM TREINO E TESTE (RETIRAR COLONA CMAX, MH E TIMEEXEC (NÃO É UMA FEATURE))\n",
    "dados_problema=dados_problema.drop(columns=['CMAX'])\n",
    "dados_problema=dados_problema.drop(columns=['MH'])\n",
    "dados_problema=dados_problema.drop(columns=['TIMEEXEC'])\n",
    "#print (dados_problema)\n",
    "\n",
    "X = dados_problema.drop(columns=['NUMGEN'])  #VARIÁVEIS INDEPENDENTES\n",
    "y = dados_problema['NUMGEN'] #VARIÁVEL DEPENDENTE\n",
    "\n",
    "#COLUNA OBJFUNX: PASSAR CMAX = 0 E WT=1 - NECESSÁRIO EM ALGUMAS TÉCNICAS DE MACHINE LEARNING\n",
    "X['OBJFUNC'] = X['OBJFUNC'].map({'Cmax': 0, 'WT': 1})\n",
    "\n",
    "#DIVIDIR OS DADOS EM CONJUNTO DE TREINO E TESTE \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'criterion': 'squared_error', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\n",
      "Metricas de Avaliação:\n",
      "MSE (Mean Squared Error): 14.3702\n",
      "RMSE (Root Mean Squared Error): 3.7908\n",
      "MAE (Mean Absolute Error): 2.7115\n",
      "MAPE (Mean Absolute Percentage Error): 0.98%\n",
      "R² Score: 0.9991\n",
      "\n",
      "Importância das Features:\n",
      "     Feature  Importance\n",
      "0      NJOBS    0.999785\n",
      "2    CMAXOPT    0.000151\n",
      "1  NMACHINES    0.000037\n",
      "3    OBJFUNC    0.000027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#================ COMPARAÇÃO REAL VS PREVISTO ==================\\nprint(\"\\n=== Comparação dos 8 primeiros exemplos ===\")\\ncomparacao = pd.DataFrame({\\'Real\\': y_test[:8].values, \\'Previsto\\': y_pred[:8]})\\ncomparacao = comparacao.round(4)\\nprint(comparacao)\\n\\nfrom sklearn.tree import plot_tree\\n\\n# ====================== PLOT DA ÁRVORE DE DECISÃO ======================\\nplt.figure(figsize=(20, 10))\\nplot_tree(\\n    best_dt, \\n    feature_names=feature_names, \\n    filled=True, \\n    rounded=True, \\n    fontsize=10,\\n    max_depth=3  # Opcional: mostra até um certo nível da árvore\\n)\\nplt.title(\"Árvore de Decisão - Visualização\")\\nplt.tight_layout()\\nplt.show()\\n#plt.savefig(\"arvore_decisao.png\", dpi=300)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ÁRVORE DE DECISÃO -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#DEFINIR PARÂMETROS\n",
    "param_grid = {\n",
    "    'criterion': ['squared_error', 'absolute_error'], \n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 10, 15],\n",
    "}\n",
    "\n",
    "#CRIAR MODELO DE ÁRVORE DE DECISÃO\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "#GRIDSEAECHCV COM VALIDAÇÃO CRUZADA \n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_regressor, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,  \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "#TREINAR O MODELO À PROCURA DOS MELHOR HIPERPARÂMETROS\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#OBTER O MELHOR MODELO\n",
    "best_dt = grid_search.best_estimator_\n",
    "print(\"Melhores Hiperparâmetros:\", grid_search.best_params_)\n",
    "\n",
    "#FAZER PREVISÕES\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "#CALCULAR MÉTRICAS DE AVALIAÇÃO\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "#RESULTADOS MÉTRICAS DE AVALIAÇÃO\n",
    "print(\"\\nMetricas de Avaliação:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse:.4f}\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "#IMPORTÂNCIA DAS FEATURES\n",
    "importances = best_dt.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nImportância das Features:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae57b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Hiperparâmetros: {'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Metricas de Avaliação:\n",
      "MSE (Mean Squared Error): 138.5621\n",
      "RMSE (Root Mean Squared Error): 11.7712\n",
      "MAE (Mean Absolute Error): 5.3146\n",
      "MAPE (Mean Absolute Percentage Error): 3.96%\n",
      "R² Score: 0.9910\n",
      "\n",
      "Importância das Features:\n",
      "     Feature  Importance\n",
      "0      NJOBS      0.5198\n",
      "2    CMAXOPT      0.4801\n",
      "3    OBJFUNC      0.0001\n",
      "1  NMACHINES      0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Plot\\nplt.figure(figsize=(10, 6))\\nsns.barplot(x=\\'Importance\\', y=\\'Feature\\', data=importance_df, palette=\\'viridis\\')\\nplt.title(\\'Importância das Features - Decision Tree\\')\\nplt.tight_layout()\\nplt.show()\\n\\n#================ COMPARAÇÃO REAL VS PREVISTO ==================\\nprint(\"\\n=== Comparação dos 8 primeiros exemplos ===\")\\ncomparacao = pd.DataFrame({\\'Real\\': y_test[:8].values, \\'Previsto\\': y_pred[:8]})\\ncomparacao = comparacao.round(4)\\nprint(comparacao)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RANDOM FOREST -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#DEFINIR PARÂMETROS\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 10, 20, 30, None],  \n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "#CRIAR MODELO RANDOM FOREST\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "#GRID SEARCHCV COM VALIADAÇÃO CRUZADA\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_regressor, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,  \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "#TREINAR O MODELO À PROCURA DOS MELHORES HIPERPARÂMETROS \n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#OBTER O MELHOR MODELO\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Melhores Hiperparâmetros:\", grid_search.best_params_)\n",
    "\n",
    "#PREVISÕES\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "#CALCULAR MÉTRICAS DE AVALIAÇÃO\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "#RESULTADOS \n",
    "print(\"\\nMetricas de Avaliação:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse:.4f}\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "print(f\"MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "#================ IMPORTÂNCIA DAS FEATURES ==================\n",
    "importances = best_rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "importance_df['Importance'] = importance_df['Importance'].round(4)\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nImportância das Features:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd39c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÉTRICAS DE AVALIAÇÃO:\n",
      "MSE: 421.5533\n",
      "RMSE: 20.5318\n",
      "MAE: 16.4200\n",
      "MAPE: 14.2170%\n",
      "R²: 0.9727\n"
     ]
    }
   ],
   "source": [
    "#REGRESSÃO LINEAR -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#MODELO DE REGRESSÃO LINEAR\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "#PREVISÕES\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "#CÁLCULO DAS MÉTRICAS DE AVALIAÇÃO\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, y_test_pred) * 100\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# RESULTADOS\n",
    "print(\"\\nMÉTRICAS DE AVALIAÇÃO:\")\n",
    "print(f\"MSE: {test_mse:.4f}\")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "print(f\"MAPE: {test_mape:.4f}%\")\n",
    "print(f\"R²: {test_r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
